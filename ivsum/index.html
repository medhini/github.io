<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight: 300;
		font-size: 18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}

	h1 {
		font-size: 32px;
		font-weight: 300;
	}

	.disclaimerbox {
		background-color: #eee;
		border: 1px solid #eeeeee;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
	}

	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
	}

	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
	}

	a:link,
	a:visited {
		color: #1367a7;
		text-decoration: none;
	}

	a:hover {
		color: #208799;
	}

	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}

	.layered-paper-big {
		/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
			0px 0px 1px 1px rgba(0, 0, 0, 0.35),
			/* The top layer shadow */
			5px 5px 0 0px #fff,
			/* The second layer */
			5px 5px 1px 1px rgba(0, 0, 0, 0.35),
			/* The second layer shadow */
			10px 10px 0 0px #fff,
			/* The third layer */
			10px 10px 1px 1px rgba(0, 0, 0, 0.35),
			/* The third layer shadow */
			15px 15px 0 0px #fff,
			/* The fourth layer */
			15px 15px 1px 1px rgba(0, 0, 0, 0.35),
			/* The fourth layer shadow */
			20px 20px 0 0px #fff,
			/* The fifth layer */
			20px 20px 1px 1px rgba(0, 0, 0, 0.35),
			/* The fifth layer shadow */
			25px 25px 0 0px #fff,
			/* The fifth layer */
			25px 25px 1px 1px rgba(0, 0, 0, 0.35);
		/* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big {
		/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
			0px 0px 1px 1px rgba(0, 0, 0, 0.35);
		/* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper {
		/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
			0px 0px 1px 1px rgba(0, 0, 0, 0.35),
			/* The top layer shadow */
			5px 5px 0 0px #fff,
			/* The second layer */
			5px 5px 1px 1px rgba(0, 0, 0, 0.35),
			/* The second layer shadow */
			10px 10px 0 0px #fff,
			/* The third layer */
			10px 10px 1px 1px rgba(0, 0, 0, 0.35);
		/* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}

	hr {
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>

<head>
	<title>TL;DW? Summarizing Instructional Videos with Task Relevance & Cross-Modal Saliency</title>
	<meta property="og:image" content="resources/teaser.png" />
	<!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title"
		content="TL;DW? Summarizing Instructional Videos with Task Relevance \& Cross-Modal Saliency" />
	<meta property="og:description"
		content="We introduce an approach for summarizing instrucitonal videos without relying on manual annotations." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<!-- <script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script> -->
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">TL;DW? Summarizing Instructional Videos with Task Relevance & Cross-Modal
			Saliency</span>
		<br><br>
		<span style="font-size:30px">ECCV 2022</span>
		<br><br>
		<table align=center width=900px>
			<table align=center width=1100px>
				<tr>
					<td align=center width=350px>
						<span style="font-size:24px"><a href="https://people.eecs.berkeley.edu/~medhini/">Medhini
								Narasimhan</a></span>&nbsp;&nbsp;&nbsp;
						<span style="font-size:24px"><a href="">Arsha
								Nagrani</a></span>&nbsp;&nbsp;&nbsp;
						<span style="font-size:24px"><a href="">Chen
								Sun</a></span>&nbsp;&nbsp;&nbsp;
						<span style="font-size:24px"><a href="">Michael
								Rubinstein</a></span>&nbsp;&nbsp;&nbsp; <br>
						<span style="font-size:24px"><a href="https://people.eecs.berkeley.edu/~trevor/">Trevor
								Darrell</a></span>&nbsp;&nbsp;&nbsp;
						<span style="font-size:24px"><a href="https://anna-rohrbach.net/">Anna
								Rohrbach</a></span>&nbsp;&nbsp;&nbsp;
						<span style="font-size:24px"><a href="">Cordelia
								Schmid</a></span>&nbsp;&nbsp;&nbsp;
					</td>
				</tr>
			</table>
			<table align=center width=500px>
				<tr>
					<td align=center width=100px>
						<center>
							<span style="font-size:20px">UC Berkeley</span>&nbsp;&nbsp;&nbsp;
							<span style="font-size:20px">Google Research</span>&nbsp;&nbsp;&nbsp;
							<span style="font-size:20px">Brown University</span>&nbsp;&nbsp;&nbsp;
						</center>
					</td>
				</tr>

			</table>
			<table align=center width=350px>
				<tr>
					<td align=center width=100px> <span style="font-size:15pt">
							<center>
								<a href="">[Paper]</a>

							</center>
					</td>
					<td align=center width=100px> <span style="font-size:15pt">
							<center>
								<a href="">[Bibtex]</a>
							</center>
					</td>
					<td align=center width=100px> <span style="font-size:15pt">
							<center><a href='https://github.com/medhini/Instructional-Video-Summarization'>[GitHub]</a>
							</center>
					</td>
				</tr>
			</table>
		</table>
	</center>

	<br>
	<center>
		<table align=center width=850px>
			<tr>
				<td align=center width=850px>
					<center>
						<img class="round" style="width:650px" align="middle" src="resources/overview.png" />
					</center>
				</td>
			</tr>
		</table>
		<table align=center width=850px>
			<tr>
				<td>
					<b>Summarizing Instructional Videos</b>Too Long; Didn't Watch? (TL;DW?) We introduce an approach for
					creating short visual summaries comprising important steps that are most relevant to the task, as
					well as salient in the video, i.e.\ referenced in the speech. For example, given a long video on
					<i>``How to make a veggie burger''</i> as shown above, the summary comprises key steps such as
					<i>fry ingredients, blend beans</i>, and <i>fry patty</i>}
				</td>
			</tr>
		</table>
	</center>

	<hr>
	<center>
		<table align=center width=850px>
			<center>
				<h1>Abstract</h1>
			</center>
			<tr>
				<td>
					YouTube users looking for instructions for a specific task may spend a long time browsing content
					trying to find the right video that matches their needs. Creating a visual summary (abridged version
					of a video) provides viewers with a quick overview and massively reduces search time. In this work,
					we focus on summarizing <i>instructional</i> videos, an under-explored area of video summarization.
					In comparison to generic videos, instructional videos can be parsed into semantically meaningful
					segments that correspond to important steps of the demonstrated task. Existing video summarization
					datasets rely on manual frame-level annotations, making them subjective and limited in size. To
					overcome this, we first automatically generate <i>pseudo summaries</i> for a corpus of instructional
					videos by exploiting two key assumptions: (i) relevant steps are likely to appear in multiple videos
					of the same task (<i>Task Relevance</i>), and (ii) they are more likely to be described by the
					demonstrator verbally (<i>Cross-Modal Saliency</i>). We propose an instructional video summarization
					network that combines a context-aware temporal video encoder and a segment scoring transformer.
					Using pseudo summaries as weak supervision, our network constructs a visual summary for an
					instructional video given only video and transcribed speech. To evaluate our model, we collect a
					high-quality test set, <i>WikiHow Summaries</i>, by scraping WikiHow articles that contain video
					demonstrations and visual depictions of steps allowing us to obtain the ground-truth summaries. We
					outperform several baselines and a state-of-the-art video summarization model on this new benchmark.
				</td>
			</tr>
		</table>
	</center>
	<br>

	<hr>
	<center>
		<table align=center width=850px>
			<center>
				<h1>Datasets</h1>
				<tr>
					<td align=center width=850px>
						<center>
							<img class="round" style="width:850px" align="middle" src="resources/dataset.png" />
						</center>
					</td>

				</tr>
				<tr>
					<td>
						<b>Pseudo Summaries.</b>
					</td>
				</tr>
				<tr>
					<td align=center width=850px>
						<center>
							<img class="round" style="width:850px" align="middle"
								src="resources/wikihowsummaries.png" />
						</center>
					</td>
				</tr>
				<tr>
					<td>
						<b>WikiHow Summaries.</b>
					</td>
				</tr>

			</center>
		</table>
	</center>

	<hr>
	<center>
		<table align=center width=850px>
			<center>
				<h1>Method</h1>
				<tr>
					<td align=center width=850px>
						<center>
							<img class="round" style="width:850px" align="middle" src="resources/approach.png" />
						</center>
					</td>
				</tr>
			</center>

			<tr>
				<td>
					<b>Overview of Instructional Video Summarization.</b> We first obtain pseudo summaries for a large
					collection of videos using our weakly supervised algorithm. Next, using the pseudo summaries as
					weak-supervision, we train our Instructional Video Summarizer (IV-Sum). It takes an input video
					along with the corresponding ASR transcript and learns to assign importance scores to each segment
					in the video. The final summary is a compilation of the high scoring video segments.
				</td>
			</tr>
		</table>
	</center>
	<br>

	<hr>
	<table align=center width=850px>
		<center>
			<h1>Qualitative Results</h1>
			<p align="center">
				<iframe width="760" height="480" src="https://www.youtube.com/embed/Orv_Jx5j5bA" frameborder="0"
					allow="autoplay; encrypted-media" allowfullscreen align="center"></iframe>
			</p>
		</center>
	</table>
	<br>

	<hr>

	<table align=center width=500 px>
		<center>
			<h1>Paper</h1>
		</center>
		<tr>
			<td><a href=""><img class="layered-paper-big" style="height:175px" src="resources/paper.png" /></a></td>
			<td><span style="font-size:12pt">M. Narasimhan, A. Nagrani, C. Sun, M. Rubinstein, T. Darrell, A. Rohrbach,
					C. Schmid.</span><br>
				<b><span style="font-size:12pt">TL;DW? Summarizing Instructional Videos with<br>Task Relevance &
						Cross-Modal Saliency</b></span><br>
				<span style="font-size:12pt">ECCV, 2022.</span>
			</td>
			</td>
		</tr>
	</table>
	<br>
	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt">
					<center>
						<a href="hhttps://arxiv.org/pdf/2208.06773.pdf">[Paper]</a> |
						<a href="https://link.springer.com/chapter/10.1007/978-3-031-19830-4_31#citeas">[Bibtex]</a>
					</center>
			</td>
		</tr>
	</table>
	<br>
	<hr>

	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center>
						<h1>Acknowledgements</h1>
					</center> We thank Daniel Fried and Bryan Seybold for valuable discussions and feedback on the
					draft. This work was supported in part by DoD including DARPA's LwLL, PTG and/or SemaFor programs,
					as well as BAIR's industrial alliance programs.
				</left>
			</td>
		</tr>
	</table>

	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
		<tr>
			<td>
				<br>
				<p align="right">
					<font size="2">
						<a href="https://github.com/richzhang/webpage-template">Template cloned from here!</a>
					</font>
				</p>
			</td>
		</tr>
	</table>


	<br>
</body>

</html>