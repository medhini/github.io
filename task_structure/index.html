<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight: 300;
		font-size: 18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}

	h1 {
		font-size: 32px;
		font-weight: 300;
	}

	.disclaimerbox {
		background-color: #eee;
		border: 1px solid #eeeeee;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
	}

	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
	}

	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
	}

	a:link,
	a:visited {
		color: #1367a7;
		text-decoration: none;
	}

	a:hover {
		color: #208799;
	}

	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}

	.layered-paper-big {
		/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
			0px 0px 1px 1px rgba(0, 0, 0, 0.35),
			/* The top layer shadow */
			5px 5px 0 0px #fff,
			/* The second layer */
			5px 5px 1px 1px rgba(0, 0, 0, 0.35),
			/* The second layer shadow */
			10px 10px 0 0px #fff,
			/* The third layer */
			10px 10px 1px 1px rgba(0, 0, 0, 0.35),
			/* The third layer shadow */
			15px 15px 0 0px #fff,
			/* The fourth layer */
			15px 15px 1px 1px rgba(0, 0, 0, 0.35),
			/* The fourth layer shadow */
			20px 20px 0 0px #fff,
			/* The fifth layer */
			20px 20px 1px 1px rgba(0, 0, 0, 0.35),
			/* The fifth layer shadow */
			25px 25px 0 0px #fff,
			/* The fifth layer */
			25px 25px 1px 1px rgba(0, 0, 0, 0.35);
		/* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big {
		/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
			0px 0px 1px 1px rgba(0, 0, 0, 0.35);
		/* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper {
		/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
			0px 0px 1px 1px rgba(0, 0, 0, 0.35),
			/* The top layer shadow */
			5px 5px 0 0px #fff,
			/* The second layer */
			5px 5px 1px 1px rgba(0, 0, 0, 0.35),
			/* The second layer shadow */
			10px 10px 0 0px #fff,
			/* The third layer */
			10px 10px 1px 1px rgba(0, 0, 0, 0.35);
		/* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}

	hr {
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>

<head>
	<title>Learning and Verification of Task Structure in Instructional Videos</title>
	<meta property="og:image" content="resources/model.png" />
	<!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title"
		content="Learning and Verification of Task Structure in Instructional Videos" />
	<meta property="og:description"
		content="We introduce an approach for summarizing instrucitonal videos without relying on manual annotations." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<!-- <script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script> -->
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">Learning and Verification of Task Structure in Instructional Videos</span>
		<br><br>
		<table align=center width=900px>
			<table align=center width=1100px>
				<tr>
					<td align=center width=350px>
						<span style="font-size:25px"><a href="https://people.eecs.berkeley.edu/~medhini/">Medhini
								Narasimhan</a><sup>1,2</sup></span>&nbsp;&nbsp;&nbsp;
						<span style="font-size:25px"><a href="https://lichengunc.github.io/">Licheng
								Yu</a><sup>2</sup></span>&nbsp;&nbsp;&nbsp;
						<span style="font-size:25px"><a href="https://ai.facebook.com/people/sean-bell">Sean
								Bell</a><sup>2</sup></span>&nbsp;&nbsp;&nbsp;
						<span style="font-size:25px"><a href="https://n-zhang.github.io/">Ning
								Zhang</a><sup>2</sup></span>&nbsp;&nbsp;&nbsp;
						<span style="font-size:25px"><a href="https://people.eecs.berkeley.edu/~trevor/">Trevor
								Darrell</a><sup>1</sup></span>&nbsp;&nbsp;&nbsp;
					</td>
				</tr>
			</table>
			<br>
			<table align=center width=500px>
				<tr>
					<td align=center width=100px>
						<center>
							<span style="font-size:23px">UC Berkeley<sup>1</sup></span>&nbsp;&nbsp;&nbsp;
							<span style="font-size:23px">Meta AI Research<sup>2</sup></span>&nbsp;&nbsp;&nbsp;
						</center>
					</td>
				</tr>
			</table>
			<br>
			<span style="font-size:27px">arXiv 2023</span>
			<br><br>
			<table align=center width=350px>
				<tr>
					<td align=center width=100px> <span style="font-size:15pt">
							<center>
								<a href="https://arxiv.org/abs/2303.13519.pdf">[Paper]</a>

							</center>
					</td>
					<td align=center width=100px> <span style="font-size:15pt">
							<center>
								<a href="https://arxiv.org/abs/2303.13519">[Bibtex]</a>
							</center>
					</td>
					<!-- <td align=center width=100px> <span style="font-size:15pt"> -->
							<!-- <center><a href='https://github.com/medhini/task-structure'>[GitHub]</a> -->
							<!-- </center> -->
					<!-- </td> -->
				</tr>
			</table>
		</table>
	</center>

	<br>
	<center>
		<p align="center">
			<iframe width="760" height="480" src="https://www.youtube.com/embed/j9HlnqD1W7w" frameborder="0"
					allow="autoplay; encrypted-media" allowfullscreen align="center"></iframe>
		</p>
	</center>

	<br>
	<center>
		<table align=center width=850px>
			<tr>
				<td>
					We introduce <b>VideoTaskformer</b>, a transformer model for learning representations of steps in instructional videos. Prior works learn step representations from single short video clips, independent of the task, thus lacking knowledge of task structure. Our model, VideoTaskformer, learns step representations for masked video steps through the global context of all surrounding steps in the video, making our learned representations aware of task semantics and structure. We use the learned representations to detect mistakes in steps and ordering of steps in new instructional videos.  
				</td>
			</tr>
		</table>
	</center>

	<hr>
	<center>
		<table align=center width=850px>
			<center>
				<h1>Abstract</h1>
			</center>
			<tr>
				<td>
					Given the enormous number of instructional videos available online, learning a diverse array of multi-step task models from videos is an appealing goal. We introduce a new pre-trained video model, VideoTaskformer, focused on representing the semantics and structure of instructional videos. We pre-train VideoTaskformer using a simple and effective objective: predicting weakly supervised textual labels for steps that are randomly masked out from an instructional video (masked step modeling). Compared to prior work which learns step representations locally, our approach involves learning them globally, leveraging video of the entire surrounding task as context. From these learned representations, we can verify if an unseen video correctly executes a given task, as well as forecast which steps are likely to be taken after a given step. We introduce two new benchmarks for detecting mistakes in instructional videos, to verify if there is an anomalous step and if steps are executed in the right order. We also introduce a long-term forecasting benchmark, where the goal is to predict long-range future steps from a given step. Our method outperforms previous baselines on these tasks, and we believe the tasks will be a valuable way for the community to measure the quality of step representations.  Additionally, we evaluate VideoTaskformer on 3 existing benchmarks---procedural activity recognition, step classification, and step forecasting---and demonstrate on each that our method outperforms existing baselines and achieves new state-of-the-art performance.
				</td>
			</tr>
		</table>
	</center>
	<br>

	<hr>
	<center>
		<table align=center width=850px>
			<center>
				<h1>Method</h1>
				<tr>
					<td align=center width=850px>
						<center>
							<img class="round" style="width:850px" align="middle" src="resources/model.png" />
						</center>
					</td>
				</tr>
			</center>
			<tr>
				<td>
					<br>
					<br>
					<b>VideoTaskformer Pre-training (Left).</b> VideoTaskformer <span class="math inline"><em>f</em><sub>VT</sub></span> learns step representations for the masked out video clip <span class="math inline"><em>v</em><sub>i</sub></span>, while attending to the other clips in the video. It consists of a video encoder <span class="math inline"><em>f</em><sub>vid</sub></span>, a step transformer <span class="math inline"><em>f</em><sub>trans</sub></span>, and a linear layer <span class="math inline"><em>f</em><sub>head</sub></span>, and is trained using weakly supervised step labels. <b>Downstream Tasks (Right).</b> We evaluate step representations learned from VideoTaskformer on 6 downstream tasks.
				</td>
			</tr>
		</table>
	</center>
	<br>

	<hr>
	<table align=center width=850px>
		<center>
			<h1>Qualitative Results</h1>
			<tr>
				<td align=center width=850px>
					<center>
						<img class="round" style="width:850px" align="middle" src="resources/res.png" />
					</center>
				</td>
			</tr>
			<tr>
				<td>
					<br>
					<br>
					We show qualitative results of our method on 4 tasks. The step labels are not used during training and are only shown here for illustrative purposes. (A) shows a result on mistake step detection, where our model's input is the sequence of video clips on the left and it correctly predicts the index of the mistake step "2" as the output. In (B), the order of the first two steps is swapped and our model classifies the sequence as incorrectly ordered. In (C), for the long-term forecasting task, the next 5 steps predicted by our model match the ground truth and in (D), for the short-term forecasting task, the model predicts the next step correctly given the past 2 steps. 
				</td>
			</tr>
		</center>
	</table>
	<br>

	<hr>

	<table align=center width=700 px>
		<center>
			<h1>Paper</h1>
		</center>
		<tr>
			<td><a href=""><img class="layered-paper-big" style="height:175px" src="resources/paper.png" /></a></td>
			<td><span style="font-size:12pt">M. Narasimhan, L. Yu, S. Bell, N. Zhang, T. Darrell</span><br>
				<b><span style="font-size:12pt">Learning and Verification of Task Structure in Instructional Videos</span></b><br>
				<span style="font-size:12pt">arXiv, 2023.</span>
			</td>
			</td>
		</tr>
	</table>
	<br>
	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt">
					<center>
						<a href="https://arxiv.org/abs/2303.13519.pdf">[Paper]</a> |
						<a href="https://arxiv.org/abs/2303.13519">[Bibtex]</a>
					</center>
			</td>
		</tr>
	</table>
	<br>
	<hr>

	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center>
						<h1>Acknowledgements</h1>
					</center> We would like to thank Suvir Mirchandani for his help with experiments and paper writing. This work was supported in part by DoD including DARPA's LwLL, PTG and/or SemaFor programs, as well as BAIR's industrial alliance programs.
				</left>
			</td>
		</tr>
	</table>

	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
		<tr>
			<td>
				<br>
				<p align="right">
					<font size="2">
						<a href="https://github.com/richzhang/webpage-template">Template cloned from here!</a>
					</font>
				</p>
			</td>
		</tr>
	</table>


	<br>
</body>

</html>