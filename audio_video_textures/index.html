<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight: 300;
		font-size: 18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}

	h1 {
		font-size: 32px;
		font-weight: 300;
	}

	.disclaimerbox {
		background-color: #eee;
		border: 1px solid #eeeeee;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
	}

	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
	}

	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
	}

	a:link,
	a:visited {
		color: #1367a7;
		text-decoration: none;
	}

	a:hover {
		color: #208799;
	}

	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}

	.layered-paper-big {
		/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
			0px 0px 1px 1px rgba(0, 0, 0, 0.35),
			/* The top layer shadow */
			5px 5px 0 0px #fff,
			/* The second layer */
			5px 5px 1px 1px rgba(0, 0, 0, 0.35),
			/* The second layer shadow */
			10px 10px 0 0px #fff,
			/* The third layer */
			10px 10px 1px 1px rgba(0, 0, 0, 0.35),
			/* The third layer shadow */
			15px 15px 0 0px #fff,
			/* The fourth layer */
			15px 15px 1px 1px rgba(0, 0, 0, 0.35),
			/* The fourth layer shadow */
			20px 20px 0 0px #fff,
			/* The fifth layer */
			20px 20px 1px 1px rgba(0, 0, 0, 0.35),
			/* The fifth layer shadow */
			25px 25px 0 0px #fff,
			/* The fifth layer */
			25px 25px 1px 1px rgba(0, 0, 0, 0.35);
		/* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big {
		/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
			0px 0px 1px 1px rgba(0, 0, 0, 0.35);
		/* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper {
		/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
			0px 0px 1px 1px rgba(0, 0, 0, 0.35),
			/* The top layer shadow */
			5px 5px 0 0px #fff,
			/* The second layer */
			5px 5px 1px 1px rgba(0, 0, 0, 0.35),
			/* The second layer shadow */
			10px 10px 0 0px #fff,
			/* The third layer */
			10px 10px 1px 1px rgba(0, 0, 0, 0.35);
		/* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}

	hr {
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>

<head>
	<title>Strumming to the Beat: Audio-Conditioned Contrastive Video Textures</title>
	<meta property="og:image" content="resources/approach.png" />
	<!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Strumming to the Beat: Audio-Conditioned Contrastive Video Textures" />
	<meta property="og:description"
		content="We  introduce  a  non-parametric  approach  for  infinitevideo texture synthesis using a representation learned viacontrastive learning." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<!-- <script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script> -->
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">Strumming to the Beat: Audio-Conditioned Contrastive Video Textures</span>
		<br><br>
		<span style="font-size:30px">WACV 2022</span>
		<br><br>
		<table align=center width=1100px>
			<tr>
				<td align=center width=250px>
					<span style="font-size:24px"><a href="https://people.eecs.berkeley.edu/~medhini/">Medhini
							Narasimhan</a><sup>1</sup></span>&nbsp;&nbsp;&nbsp;
					<span style="font-size:24px"><a href="https://people.eecs.berkeley.edu/~shiry/">Shiry
							Ginosar</a><sup>1</sup></span>&nbsp;&nbsp;&nbsp;
					<span style="font-size:24px"><a href="http://andrewowens.com/">Andrew
							Owens</a><sup>2</sup></span>&nbsp;&nbsp;&nbsp;
					<span style="font-size:24px"><a href="https://people.eecs.berkeley.edu/~efros/">Alexei
							Efros</a><sup>1</sup></span>&nbsp;&nbsp;&nbsp;
					<span style="font-size:24px"><a href="https://people.eecs.berkeley.edu/~trevor/">Trevor
							Darrell</a><sup>1</sup></span>&nbsp;&nbsp;&nbsp;
				</td>
			</tr>
		</table>
		<table align=center width=500px>
			<tr>
				<td align=center width=100px>
					<center>
						<span style="font-size:20px"><sup>1</sup>UC Berkeley</span>
					</center>
				</td>
				<td align=center width=100px>
					<center>
						<span style="font-size:20px"><sup>2</sup>University of Michigan</span>
					</center>
				</td>
			</tr>

		</table>
		<table align=center width=350px>
			<tr>
				<td align=center width=100px> <span style="font-size:15pt">
						<center>
							<a href="https://arxiv.org/pdf/2104.02687.pdf">[Paper]</a>

						</center>
				</td>
				<td align=center width=100px> <span style="font-size:15pt">
						<center><a href='https://github.com/medhini/audio-video-textures'>[GitHub]</a></center>
				</td>
			</tr>
		</table>
		</table>
	</center>

	<center>
		<p align="center">
			<iframe width="760" height="480" src="https://www.youtube.com/embed/JCuEbSF4kxU" frameborder="0"
				allow="autoplay; encrypted-media" allowfullscreen align="center"></iframe>
		</p>
	</center>

	<hr>

	<table align=center width=850px>
		<center>
			<h1>Abstract</h1>
		</center>
		<tr>
			<td>
				We introduce a non-parametric approach for infinite video texture synthesis using a representation
				learned via contrastive learning. We take inspiration from <a
					href="https://www.cc.gatech.edu/cpl/projects/videotexture/">Video Textures</a>, which showed that
				plausible new videos could be generated from a single one by stitching its frames together in a novel
				yet consistent order. This classic work, however, was constrained by its use of hand-designed distance
				metrics, limiting its use to simple, repetitive videos. We draw on recent techniques from
				self-supervised learning to learn this distance metric, allowing us to compare frames in a mannerthat
				scales to more challenging dynamics, and to condition on other data, such as audio. We learn
				representations for video frames and frame-to-frame transition probabilities by fitting a video-specific
				model trained using contrastive learning. To synthesize a texture, we randomly sample frames with high
				transition probabilities to generate diverse temporally smooth videos with novel sequences and
				transitions. The model naturally extends to an audio-conditioned setting without requiring any
				finetuning. Our model outperforms baselines on human perceptual scores, can handle a diverse range of
				input videos, and can combine semantic and audio-visual cues in order to synthesize videos that
				synchronize well with an audio signal.
			</td>
		</tr>
	</table>
	<br>

	<!-- <hr>
	<center>
		<h1>More Qualitative Results</h1>
	</center> -->

	<hr>

	<table align=center width=500 px>
		<center>
			<h1>Paper</h1>
		</center>
		<tr>
			<td><a href="https://arxiv.org/pdf/2104.02687.pdf"><img class="layered-paper-big" style="height:175px"
						src="resources/paper.png" /></a></td>
			<td><span style="font-size:12pt">M. Narasimhan, S. Ginosar, A. Owens, <br> A. A. Efros, T.
					Darrell.</span><br>
				<b><span style="font-size:12pt">Strumming to the Beat:<br>Audio-Conditioned Contrastive <br> Video
						Textures.</b></span><br>
				<!-- <span style="font-size:12pt">arXiv, 2022.</span> -->
			</td>
		</tr>
	</table>
	<br>
	<br>
	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt">
					<center>
						<a href="https://arxiv.org/pdf/2104.02687.pdf">[Paper]</a> |
						<a
							href="https://www.computer.org/csdl/proceedings-article/wacv/2022/091500a507/1B13PAZpSTe">[Bibtex]</a>
					</center>
			</td>
		</tr>
	</table>
	<br>
	<hr>

	<br>


	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center>
						<h1>Acknowledgements</h1>
					</center> We thank Arun Mallya, Allan Jabri, Anna Rohrbach,
					Amir Bar, Suzie Petryk, and Parsa Mahmoudieh for very helpful discussions and
					feedback. This work was supported in part by DoD including DARPA's XAI, LwLL, and
					SemaFor programs, as well as BAIR's industrial alliance programs.
				</left>
			</td>
		</tr>
	</table>

	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
		<tr>
			<td>
				<br>
				<p align="right">
					<font size="2">
						<a href="https://github.com/richzhang/webpage-template">Template cloned from here!</a>
					</font>
				</p>
			</td>
		</tr>
	</table>


	<br>
</body>

</html>